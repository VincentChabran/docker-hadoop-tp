{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337c358ee36c9019",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.583376Z",
     "start_time": "2024-11-20T21:08:00.553146800Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.sql import functions as F\n",
    "# import sys\n",
    "# from pyspark.sql import SparkSession\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pyspark.sql import functions as F\n",
    "# import seaborn as sns\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Data Processing dans catalogue csv\") \\\n",
    "#     .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# print(\"Session Spark initialisée avec succès.\")\n",
    "# spark.sql(\"USE concessionnaire\")\n",
    "# catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.584880800Z",
     "start_time": "2024-11-20T21:08:00.558036200Z"
    }
   },
   "id": "5675739878b8a856"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# catalogue_df.show(10)\n",
    "# catalogue_df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.584880800Z",
     "start_time": "2024-11-20T21:08:00.566071900Z"
    }
   },
   "id": "eaa41c7f1a0ddbf1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576765ac9d2156b6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.616886500Z",
     "start_time": "2024-11-20T21:08:00.574542500Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import when, col, lit\n",
    "# from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# \n",
    "# # Initialisation de la session Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Clustering Véhicules\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# print(\"Session Spark initialisée avec succès.\")\n",
    "# spark.sql(\"USE concessionnaire\")\n",
    "# \n",
    "# # Charger les données\n",
    "# catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "# print(f\"Nombre de lignes dans le DataFrame initial : {catalogue_df.count()}\")\n",
    "# catalogue_df.show(5, truncate=False)\n",
    "# \n",
    "# # Étape 0 : Correction des anomalies dans 'rejets_co2'\n",
    "# \n",
    "# # Identifier les anomalies\n",
    "# anomalies_df = catalogue_df.filter(\n",
    "#     (col('rejets_co2').isNull()) |\n",
    "#     (col('rejets_co2') == 0) |\n",
    "#     ((col('unified_horse_power') > 100) & (col('rejets_co2') < 50))\n",
    "# )\n",
    "# print(f\"Nombre d'anomalies détectées : {anomalies_df.count()}\")\n",
    "# \n",
    "# # Calcul des moyennes\n",
    "# avg_co2_by_model = F.broadcast(catalogue_df.filter(col('rejets_co2') > 0)\n",
    "#                                .groupBy('marque', 'modele')\n",
    "#                                .agg(F.avg('rejets_co2').alias('avg_rejets_co2_model')))\n",
    "# \n",
    "# avg_co2_by_marque = F.broadcast(catalogue_df.filter(col('rejets_co2') > 0)\n",
    "#                                 .groupBy('marque')\n",
    "#                                 .agg(F.avg('rejets_co2').alias('avg_rejets_co2_marque')))\n",
    "# \n",
    "# avg_co2_general = catalogue_df.filter(col('rejets_co2') > 0) \\\n",
    "#     .agg(F.avg('rejets_co2').alias('avg_rejets_co2_general')).collect()[0]['avg_rejets_co2_general']\n",
    "# \n",
    "# # Imputation des anomalies\n",
    "# anomalies_df = anomalies_df \\\n",
    "#     .join(avg_co2_by_model, on=['marque', 'modele'], how='left') \\\n",
    "#     .join(avg_co2_by_marque, on='marque', how='left') \\\n",
    "#     .withColumn('rejets_co2', when(col('avg_rejets_co2_model').isNotNull(), col('avg_rejets_co2_model'))\n",
    "#                 .when(col('avg_rejets_co2_marque').isNotNull(), col('avg_rejets_co2_marque'))\n",
    "#                 .otherwise(lit(avg_co2_general))) \\\n",
    "#     .drop('avg_rejets_co2_model', 'avg_rejets_co2_marque')\n",
    "# \n",
    "# # Mise à jour des données\n",
    "# catalogue_corrected_df = catalogue_df.join(anomalies_df.select('marque', 'modele', 'rejets_co2'), \n",
    "#                                            on=['marque', 'modele'], how='left_anti').unionByName(anomalies_df)\n",
    "# print(\"Données corrigées prêtes.\")\n",
    "# catalogue_corrected_df.show(5, truncate=False)\n",
    "# \n",
    "# # Étape 1 : Préparation des données pour le clustering\n",
    "# numerical_cols = ['prix', 'unified_horse_power', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "# clustering_df = catalogue_corrected_df.select(\n",
    "#     *[col(c).cast('double').alias(c) for c in numerical_cols],\n",
    "#     'marque', 'modele'\n",
    "# ).fillna({c: 0 for c in numerical_cols})\n",
    "# \n",
    "# assembler = VectorAssembler(inputCols=numerical_cols, outputCol='features_raw')\n",
    "# assembled_df = assembler.transform(clustering_df)\n",
    "# \n",
    "# scaler = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "# scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "# print(\"Données normalisées prêtes pour le clustering.\")\n",
    "# scaled_df.select(\"features\").show(5, truncate=False)\n",
    "# \n",
    "# # Étape 2 : Clustering avec K-Means\n",
    "# k = 9\n",
    "# kmeans = KMeans(featuresCol='features', k=k, seed=1)\n",
    "# model = kmeans.fit(scaled_df)\n",
    "# clustered_df = model.transform(scaled_df)\n",
    "# \n",
    "# # Étape 3 : Analyse des clusters\n",
    "# cluster_stats = clustered_df.groupBy('prediction').agg(\n",
    "#     F.count('*').alias('count'),\n",
    "#     F.mean('prix').alias('mean_prix'),\n",
    "#     F.mean('unified_horse_power').alias('mean_horse_power'),\n",
    "#     F.mean('rejets_co2').alias('mean_rejets_co2'),\n",
    "#     F.mean('bonus_malus').alias('mean_bonus_malus'),\n",
    "#     F.mean('nbplaces').alias('mean_nbplaces'),\n",
    "#     F.mean('nbportes').alias('mean_nbportes')\n",
    "# ).orderBy('prediction')\n",
    "# \n",
    "# print(\"Statistiques des clusters :\")\n",
    "# cluster_stats.show(truncate=False)\n",
    "# \n",
    "# # Étape 4 : Assignation des catégories\n",
    "# clustered_df = clustered_df.withColumn(\n",
    "#     \"categorie\",\n",
    "#     when(col(\"prediction\") == 0, \"Berlines hybrides haut de gamme\") \n",
    "#     .when(col(\"prediction\") == 1, \"Berlines de luxe\")\n",
    "#     .when(col(\"prediction\") == 2, \"Voitures sportives haut de gamme\")\n",
    "#     .when(col(\"prediction\") == 3, \"Citadines économiques\")\n",
    "#     .when(col(\"prediction\") == 4, \"Familiales grandes places\")\n",
    "#     .when(col(\"prediction\") == 5, \"Berlines intermédiaires thermiques\")\n",
    "#     .when(col(\"prediction\") == 6, \"Micro-citadines électriques\") \n",
    "#     .when(col(\"prediction\") == 7, \"Citadines\")\n",
    "#     .when(col(\"prediction\") == 8, \"Citadines compactes\")\n",
    "#     .otherwise(\"Autres\")\n",
    "# )\n",
    "# \n",
    "# # Étape 5 : Vérification des catégories\n",
    "# categories = clustered_df.select(\"categorie\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# for cat in categories:\n",
    "#     print(f\"\\nExemples pour la catégorie : {cat}\")\n",
    "#     clustered_df.filter(col(\"categorie\") == cat).select(\n",
    "#         \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\"\n",
    "#     ).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark initialisée avec succès.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_NOT_FOUND] The schema `concessionnaire` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClustering Véhicules\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39menableHiveSupport() \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSession Spark initialisée avec succès.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUSE concessionnaire\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Charger les données\u001B[39;00m\n\u001B[1;32m     18\u001B[0m catalogue_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM catalogue_co2_merge_processed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1627\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1628\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1629\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1630\u001B[0m         )\n\u001B[0;32m-> 1631\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1632\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1633\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [SCHEMA_NOT_FOUND] The schema `concessionnaire` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Clustering Véhicules\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(\"Session Spark initialisée avec succès.\")\n",
    "spark.sql(\"USE concessionnaire\")\n",
    "\n",
    "# Charger les données\n",
    "catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "print(f\"Nombre de lignes dans le DataFrame initial : {catalogue_df.count()}\")\n",
    "catalogue_df.show(5, truncate=False)\n",
    "\n",
    "# Étape 1 : Préparation des données pour le clustering\n",
    "numerical_cols = ['prix', 'unified_horse_power', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "clustering_df = catalogue_df.select(\n",
    "    *[col(c).cast('double').alias(c) for c in numerical_cols],\n",
    "    'marque', 'modele'\n",
    ").fillna({c: 0 for c in numerical_cols})\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol='features_raw')\n",
    "assembled_df = assembler.transform(clustering_df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "\n",
    "print(\"Données normalisées prêtes pour le clustering :\")\n",
    "scaled_df.select(\"features\").show(5, truncate=False)\n",
    "\n",
    "# Étape 2 : Clustering avec K-Means\n",
    "k = 9\n",
    "kmeans = KMeans(featuresCol='features', k=k, seed=1)\n",
    "model = kmeans.fit(scaled_df)\n",
    "clustered_df = model.transform(scaled_df)\n",
    "\n",
    "# Calcul du score de silhouette\n",
    "evaluator = ClusteringEvaluator(featuresCol='features', metricName='silhouette')\n",
    "silhouette_score = evaluator.evaluate(clustered_df)\n",
    "print(f\"Score de silhouette pour k={k}: {silhouette_score:.4f}\")\n",
    "\n",
    "# Étape 3 : Analyse des clusters\n",
    "print(\"\\n--- Répartition des clusters ---\")\n",
    "clustered_df.groupBy('prediction').count().orderBy('prediction').show()\n",
    "\n",
    "print(\"\\n--- Statistiques des clusters ---\")\n",
    "cluster_stats = clustered_df.groupBy('prediction').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    *[F.mean(c).alias(f'mean_{c}') for c in numerical_cols],\n",
    "    *[F.stddev(c).alias(f'stddev_{c}') for c in numerical_cols]\n",
    ")\n",
    "cluster_stats.show(truncate=False)\n",
    "\n",
    "# Étape 4 : Réduction de dimensions pour visualisation\n",
    "print(\"\\n--- Réduction de dimensions avec PCA ---\")\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(clustered_df)\n",
    "pca_df = pca_model.transform(clustered_df)\n",
    "\n",
    "# Collecte des données pour Matplotlib\n",
    "pca_data = pca_df.select(\"pca_features\", \"prediction\").rdd.map(\n",
    "    lambda row: (row[\"pca_features\"][0], row[\"pca_features\"][1], row[\"prediction\"])\n",
    ").collect()\n",
    "\n",
    "x_coords = [x[0] for x in pca_data]\n",
    "y_coords = [x[1] for x in pca_data]\n",
    "predictions = [x[2] for x in pca_data]\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(x_coords, y_coords, c=predictions, cmap=\"tab10\", alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(f\"Visualisation des clusters (PCA) - Silhouette Score: {silhouette_score:.4f}\")\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Étape 5 : Vérification des catégories\n",
    "print(\"\\n--- Exemples pour chaque cluster ---\")\n",
    "categories = clustered_df.select(\"prediction\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for cluster in categories:\n",
    "    print(f\"\\n--- Exemples pour le cluster {cluster} ---\")\n",
    "    clustered_df.filter(col(\"prediction\") == cluster).select(\n",
    "        \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\"\n",
    "    ).show(5, truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.662696600Z",
     "start_time": "2024-11-20T21:08:00.587584700Z"
    }
   },
   "id": "c5268fa03ee3a409"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clustered_df.groupBy(\"prediction\").count().show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.669310100Z",
     "start_time": "2024-11-20T21:08:00.664431100Z"
    }
   },
   "id": "8c3ffe82d27148c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ajouter des catégories pour chaque cluster\n",
    "clustered_df = clustered_df.withColumn(\n",
    "    \"categorie\",\n",
    "    when(col(\"prediction\") == 1, \"Berlines compactes économiques\")\n",
    "    .when(col(\"prediction\") == 6, \"Compactes premium\")\n",
    "    .when(col(\"prediction\") == 5, \"Monospaces familiaux 7 places\")\n",
    "    .when(col(\"prediction\") == 4, \"Berlines de luxe haut de gamme\")\n",
    "    .when(col(\"prediction\") == 8, \"Voitures sportives haut de gamme\")\n",
    "    .when(col(\"prediction\") == 2, \"Citadines économiques compactes\")\n",
    "    .when(col(\"prediction\") == 0, \"Berlines routières intermédiaires\")\n",
    "    .when(col(\"prediction\") == 3, \"Micro-citadines économiques\")\n",
    "    .when(col(\"prediction\") == 7, \"Monospaces compacts économiques\")\n",
    "    .otherwise(\"Non défini\")\n",
    ")\n",
    "\n",
    "# Afficher les données avec les catégories\n",
    "print(\"\\n--- Véhicules avec catégories assignées ---\")\n",
    "clustered_df.select(\n",
    "    \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\", \"categorie\"\n",
    ").show(100, truncate=False)\n",
    "\n",
    "# Sauvegarder les résultats pour un usage ultérieur\n",
    "# clustered_df.write.mode('overwrite').saveAsTable(\"concessionnaire.clustered_with_categories\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-20T21:08:00.666641400Z"
    }
   },
   "id": "a3c29363c32e9a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouper les résultats par catégorie et afficher des exemples pour chaque\n",
    "categories = clustered_df.select(\"categorie\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for cat in categories:\n",
    "    print(f\"\\n--- Exemples pour la catégorie : {cat} ---\")\n",
    "    clustered_df.filter(col(\"categorie\") == cat).select(\n",
    "        \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\", \"categorie\"\n",
    "    ).show(10, truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-20T21:08:00.669310100Z"
    }
   },
   "id": "ae3f9c8054e2877c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"hdfs://namenode:9000/models/kmeans_model\"\n",
    "model.save(model_path)\n",
    "print(f\"Modèle K-Means sauvegardé dans HDFS : {model_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-20T21:08:00.671966100Z"
    }
   },
   "id": "160a524799d5443f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Charger le modèle K-Means\n",
    "loaded_model = KMeansModel.load(model_path)\n",
    "print(\"Modèle K-Means chargé avec succès.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-20T21:08:00.673039600Z"
    }
   },
   "id": "5cdbfa4d72460885"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"K-Means Clustering avec CSV incomplet\") \\\n",
    "    .getOrCreate()\n",
    "spark.sql(\"USE concessionnaire\")\n",
    "# Charger les données d'entraînement pour calculer les moyennes\n",
    "training_data = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "\n",
    "# Calculer les moyennes des colonnes manquantes\n",
    "impute_values = {\n",
    "    col_name: training_data.select(F.mean(col_name).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "    for col_name in ['rejets_co2', 'bonus_malus']\n",
    "}\n",
    "\n",
    "# Charger les données de prédiction\n",
    "new_data_df = spark.sql(\"SELECT * FROM immatriculations_data\")\n",
    "print(f\"Nombre de lignes dans le DataFrame initial : {new_data_df.count()}\")\n",
    "\n",
    "# Colonnes nécessaires pour le modèle\n",
    "numerical_cols = ['prix', 'puissance', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "\n",
    "# Ajouter les colonnes manquantes avec des moyennes imputées\n",
    "for col_name in numerical_cols:\n",
    "    if col_name not in new_data_df.columns:\n",
    "        default_value = impute_values.get(col_name, 0)  # Utiliser 0 si aucune moyenne n'est disponible\n",
    "        print(f\"Ajout de la colonne manquante : {col_name} avec une valeur imputée : {default_value}\")\n",
    "        new_data_df = new_data_df.withColumn(col_name, lit(default_value))\n",
    "    else:\n",
    "        print(f\"Remplissage des valeurs manquantes pour : {col_name}\")\n",
    "        new_data_df = new_data_df.fillna({col_name: impute_values.get(col_name, 0)})\n",
    "\n",
    "# Préparer les données pour le modèle\n",
    "prepared_df = new_data_df.select(\n",
    "    *[col(c).cast('double').alias(c) for c in numerical_cols],\n",
    "    'marque', 'nom'  # Conserver les colonnes non numériques\n",
    ")\n",
    "\n",
    "# Assembler les colonnes en vecteur de caractéristiques\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features_raw\")\n",
    "assembled_df = assembler.transform(prepared_df)\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "normalized_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "print(\"\\n--- Données préparées et normalisées ---\")\n",
    "normalized_df.select(\"features\").show(5, truncate=False)\n",
    "\n",
    "# Charger le modèle K-Means\n",
    "kmeans_model = KMeansModel.load(\"hdfs://namenode:9000/models/kmeans_model\")\n",
    "print(\"Modèle K-Means chargé avec succès.\")\n",
    "\n",
    "# Appliquer le modèle pour obtenir les prédictions\n",
    "predicted_df = kmeans_model.transform(normalized_df)\n",
    "\n",
    "# Ajouter des catégories basées sur les prédictions\n",
    "predicted_df = predicted_df.withColumn(\n",
    "    \"categorie\",\n",
    "    when(col(\"prediction\") == 1, \"Berlines compactes économiques\")\n",
    "    .when(col(\"prediction\") == 6, \"Compactes premium\")\n",
    "    .when(col(\"prediction\") == 5, \"Monospaces familiaux 7 places\")\n",
    "    .when(col(\"prediction\") == 4, \"Berlines de luxe haut de gamme\")\n",
    "    .when(col(\"prediction\") == 8, \"Voitures sportives haut de gamme\")\n",
    "    .when(col(\"prediction\") == 2, \"Citadines économiques compactes\")\n",
    "    .when(col(\"prediction\") == 0, \"Berlines routières intermédiaires\")\n",
    "    .when(col(\"prediction\") == 3, \"Micro-citadines économiques\")\n",
    "    .when(col(\"prediction\") == 7, \"Monospaces compacts économiques\")\n",
    "    .otherwise(\"Non défini\")\n",
    ")\n",
    "\n",
    "# Afficher les résultats avec 'marque' et 'nom'\n",
    "print(\"\\n--- Résultats avec catégories assignées ---\")\n",
    "predicted_df.select(\n",
    "    \"marque\", \"nom\", *numerical_cols, \"prediction\", \"categorie\"\n",
    ").show(100, truncate=False)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "output_path = \"hdfs://namenode:9000/output/clustered_results.csv\"\n",
    "predicted_df.select(\n",
    "    \"marque\", \"nom\", *numerical_cols, \"prediction\", \"categorie\"\n",
    ").write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "print(f\"Résultats sauvegardés dans : {output_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-20T21:08:00.674641Z"
    }
   },
   "id": "cf90705a44a9c696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T21:08:00.678381800Z",
     "start_time": "2024-11-20T21:08:00.675705400Z"
    }
   },
   "id": "9267153c71b387f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
