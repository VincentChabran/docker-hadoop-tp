{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c358ee36c9019",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.sql import functions as F\n",
    "# import sys\n",
    "# from pyspark.sql import SparkSession\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pyspark.sql import functions as F\n",
    "# import seaborn as sns\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Data Processing dans catalogue csv\") \\\n",
    "#     .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# print(\"Session Spark initialisée avec succès.\")\n",
    "# spark.sql(\"USE concessionnaire\")\n",
    "# catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5675739878b8a856"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# catalogue_df.show(10)\n",
    "# catalogue_df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa41c7f1a0ddbf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576765ac9d2156b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import when, col, lit\n",
    "# from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# \n",
    "# # Initialisation de la session Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Clustering Véhicules\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# print(\"Session Spark initialisée avec succès.\")\n",
    "# spark.sql(\"USE concessionnaire\")\n",
    "# \n",
    "# # Charger les données\n",
    "# catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "# print(f\"Nombre de lignes dans le DataFrame initial : {catalogue_df.count()}\")\n",
    "# catalogue_df.show(5, truncate=False)\n",
    "# \n",
    "# # Étape 0 : Correction des anomalies dans 'rejets_co2'\n",
    "# \n",
    "# # Identifier les anomalies\n",
    "# anomalies_df = catalogue_df.filter(\n",
    "#     (col('rejets_co2').isNull()) |\n",
    "#     (col('rejets_co2') == 0) |\n",
    "#     ((col('unified_horse_power') > 100) & (col('rejets_co2') < 50))\n",
    "# )\n",
    "# print(f\"Nombre d'anomalies détectées : {anomalies_df.count()}\")\n",
    "# \n",
    "# # Calcul des moyennes\n",
    "# avg_co2_by_model = F.broadcast(catalogue_df.filter(col('rejets_co2') > 0)\n",
    "#                                .groupBy('marque', 'modele')\n",
    "#                                .agg(F.avg('rejets_co2').alias('avg_rejets_co2_model')))\n",
    "# \n",
    "# avg_co2_by_marque = F.broadcast(catalogue_df.filter(col('rejets_co2') > 0)\n",
    "#                                 .groupBy('marque')\n",
    "#                                 .agg(F.avg('rejets_co2').alias('avg_rejets_co2_marque')))\n",
    "# \n",
    "# avg_co2_general = catalogue_df.filter(col('rejets_co2') > 0) \\\n",
    "#     .agg(F.avg('rejets_co2').alias('avg_rejets_co2_general')).collect()[0]['avg_rejets_co2_general']\n",
    "# \n",
    "# # Imputation des anomalies\n",
    "# anomalies_df = anomalies_df \\\n",
    "#     .join(avg_co2_by_model, on=['marque', 'modele'], how='left') \\\n",
    "#     .join(avg_co2_by_marque, on='marque', how='left') \\\n",
    "#     .withColumn('rejets_co2', when(col('avg_rejets_co2_model').isNotNull(), col('avg_rejets_co2_model'))\n",
    "#                 .when(col('avg_rejets_co2_marque').isNotNull(), col('avg_rejets_co2_marque'))\n",
    "#                 .otherwise(lit(avg_co2_general))) \\\n",
    "#     .drop('avg_rejets_co2_model', 'avg_rejets_co2_marque')\n",
    "# \n",
    "# # Mise à jour des données\n",
    "# catalogue_corrected_df = catalogue_df.join(anomalies_df.select('marque', 'modele', 'rejets_co2'), \n",
    "#                                            on=['marque', 'modele'], how='left_anti').unionByName(anomalies_df)\n",
    "# print(\"Données corrigées prêtes.\")\n",
    "# catalogue_corrected_df.show(5, truncate=False)\n",
    "# \n",
    "# # Étape 1 : Préparation des données pour le clustering\n",
    "# numerical_cols = ['prix', 'unified_horse_power', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "# clustering_df = catalogue_corrected_df.select(\n",
    "#     *[col(c).cast('double').alias(c) for c in numerical_cols],\n",
    "#     'marque', 'modele'\n",
    "# ).fillna({c: 0 for c in numerical_cols})\n",
    "# \n",
    "# assembler = VectorAssembler(inputCols=numerical_cols, outputCol='features_raw')\n",
    "# assembled_df = assembler.transform(clustering_df)\n",
    "# \n",
    "# scaler = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "# scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "# print(\"Données normalisées prêtes pour le clustering.\")\n",
    "# scaled_df.select(\"features\").show(5, truncate=False)\n",
    "# \n",
    "# # Étape 2 : Clustering avec K-Means\n",
    "# k = 9\n",
    "# kmeans = KMeans(featuresCol='features', k=k, seed=1)\n",
    "# model = kmeans.fit(scaled_df)\n",
    "# clustered_df = model.transform(scaled_df)\n",
    "# \n",
    "# # Étape 3 : Analyse des clusters\n",
    "# cluster_stats = clustered_df.groupBy('prediction').agg(\n",
    "#     F.count('*').alias('count'),\n",
    "#     F.mean('prix').alias('mean_prix'),\n",
    "#     F.mean('unified_horse_power').alias('mean_horse_power'),\n",
    "#     F.mean('rejets_co2').alias('mean_rejets_co2'),\n",
    "#     F.mean('bonus_malus').alias('mean_bonus_malus'),\n",
    "#     F.mean('nbplaces').alias('mean_nbplaces'),\n",
    "#     F.mean('nbportes').alias('mean_nbportes')\n",
    "# ).orderBy('prediction')\n",
    "# \n",
    "# print(\"Statistiques des clusters :\")\n",
    "# cluster_stats.show(truncate=False)\n",
    "# \n",
    "# # Étape 4 : Assignation des catégories\n",
    "# clustered_df = clustered_df.withColumn(\n",
    "#     \"categorie\",\n",
    "#     when(col(\"prediction\") == 0, \"Berlines hybrides haut de gamme\") \n",
    "#     .when(col(\"prediction\") == 1, \"Berlines de luxe\")\n",
    "#     .when(col(\"prediction\") == 2, \"Voitures sportives haut de gamme\")\n",
    "#     .when(col(\"prediction\") == 3, \"Citadines économiques\")\n",
    "#     .when(col(\"prediction\") == 4, \"Familiales grandes places\")\n",
    "#     .when(col(\"prediction\") == 5, \"Berlines intermédiaires thermiques\")\n",
    "#     .when(col(\"prediction\") == 6, \"Micro-citadines électriques\") \n",
    "#     .when(col(\"prediction\") == 7, \"Citadines\")\n",
    "#     .when(col(\"prediction\") == 8, \"Citadines compactes\")\n",
    "#     .otherwise(\"Autres\")\n",
    "# )\n",
    "# \n",
    "# # Étape 5 : Vérification des catégories\n",
    "# categories = clustered_df.select(\"categorie\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# for cat in categories:\n",
    "#     print(f\"\\nExemples pour la catégorie : {cat}\")\n",
    "#     clustered_df.filter(col(\"categorie\") == cat).select(\n",
    "#         \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\"\n",
    "#     ).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Clustering Véhicules\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(\"Session Spark initialisée avec succès.\")\n",
    "spark.sql(\"USE concessionnaire\")\n",
    "\n",
    "# Charger les données\n",
    "catalogue_df = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "print(f\"Nombre de lignes dans le DataFrame initial : {catalogue_df.count()}\")\n",
    "catalogue_df.show(5, truncate=False)\n",
    "\n",
    "# Étape 1 : Préparation des données pour le clustering\n",
    "numerical_cols = ['prix', 'unified_horse_power', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "clustering_df = catalogue_df.select(\n",
    "    *[col(c).cast('double').alias(c) for c in numerical_cols],\n",
    "    'marque', 'modele'\n",
    ").fillna({c: 0 for c in numerical_cols})\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol='features_raw')\n",
    "assembled_df = assembler.transform(clustering_df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "\n",
    "print(\"Données normalisées prêtes pour le clustering :\")\n",
    "scaled_df.select(\"features\").show(5, truncate=False)\n",
    "\n",
    "# Étape 2 : Clustering avec K-Means\n",
    "k = 9\n",
    "kmeans = KMeans(featuresCol='features', k=k, seed=1)\n",
    "model = kmeans.fit(scaled_df)\n",
    "clustered_df = model.transform(scaled_df)\n",
    "\n",
    "# Calcul du score de silhouette\n",
    "evaluator = ClusteringEvaluator(featuresCol='features', metricName='silhouette')\n",
    "silhouette_score = evaluator.evaluate(clustered_df)\n",
    "print(f\"Score de silhouette pour k={k}: {silhouette_score:.4f}\")\n",
    "\n",
    "# Étape 3 : Analyse des clusters\n",
    "print(\"\\n--- Répartition des clusters ---\")\n",
    "clustered_df.groupBy('prediction').count().orderBy('prediction').show()\n",
    "\n",
    "print(\"\\n--- Statistiques des clusters ---\")\n",
    "cluster_stats = clustered_df.groupBy('prediction').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    *[F.mean(c).alias(f'mean_{c}') for c in numerical_cols],\n",
    "    *[F.stddev(c).alias(f'stddev_{c}') for c in numerical_cols]\n",
    ")\n",
    "cluster_stats.show(truncate=False)\n",
    "\n",
    "# Étape 4 : Réduction de dimensions pour visualisation\n",
    "print(\"\\n--- Réduction de dimensions avec PCA ---\")\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(clustered_df)\n",
    "pca_df = pca_model.transform(clustered_df)\n",
    "\n",
    "# Collecte des données pour Matplotlib\n",
    "pca_data = pca_df.select(\"pca_features\", \"prediction\").rdd.map(\n",
    "    lambda row: (row[\"pca_features\"][0], row[\"pca_features\"][1], row[\"prediction\"])\n",
    ").collect()\n",
    "\n",
    "x_coords = [x[0] for x in pca_data]\n",
    "y_coords = [x[1] for x in pca_data]\n",
    "predictions = [x[2] for x in pca_data]\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(x_coords, y_coords, c=predictions, cmap=\"tab10\", alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(f\"Visualisation des clusters (PCA) - Silhouette Score: {silhouette_score:.4f}\")\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Étape 5 : Vérification des catégories\n",
    "print(\"\\n--- Exemples pour chaque cluster ---\")\n",
    "categories = clustered_df.select(\"prediction\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for cluster in categories:\n",
    "    print(f\"\\n--- Exemples pour le cluster {cluster} ---\")\n",
    "    clustered_df.filter(col(\"prediction\") == cluster).select(\n",
    "        \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\"\n",
    "    ).show(5, truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5268fa03ee3a409"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clustered_df.groupBy(\"prediction\").count().show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3ffe82d27148c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ajouter des catégories pour chaque cluster\n",
    "clustered_df = clustered_df.withColumn(\n",
    "    \"categorie\",\n",
    "    when(col(\"prediction\") == 1, \"Berlines compactes économiques\")\n",
    "    .when(col(\"prediction\") == 6, \"Compactes premium\")\n",
    "    .when(col(\"prediction\") == 5, \"Monospaces familiaux 7 places\")\n",
    "    .when(col(\"prediction\") == 4, \"Berlines de luxe haut de gamme\")\n",
    "    .when(col(\"prediction\") == 8, \"Voitures sportives haut de gamme\")\n",
    "    .when(col(\"prediction\") == 2, \"Citadines économiques compactes\")\n",
    "    .when(col(\"prediction\") == 0, \"Berlines routières intermédiaires\")\n",
    "    .when(col(\"prediction\") == 3, \"Micro-citadines économiques\")\n",
    "    .when(col(\"prediction\") == 7, \"Monospaces compacts économiques\")\n",
    "    .otherwise(\"Non défini\")\n",
    ")\n",
    "\n",
    "# Afficher les données avec les catégories\n",
    "print(\"\\n--- Véhicules avec catégories assignées ---\")\n",
    "clustered_df.select(\n",
    "    \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\", \"categorie\"\n",
    ").show(100, truncate=False)\n",
    "\n",
    "# Sauvegarder les résultats pour un usage ultérieur\n",
    "# clustered_df.write.mode('overwrite').saveAsTable(\"concessionnaire.clustered_with_categories\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c29363c32e9a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouper les résultats par catégorie et afficher des exemples pour chaque\n",
    "categories = clustered_df.select(\"categorie\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for cat in categories:\n",
    "    print(f\"\\n--- Exemples pour la catégorie : {cat} ---\")\n",
    "    clustered_df.filter(col(\"categorie\") == cat).select(\n",
    "        \"marque\", \"modele\", \"prix\", \"unified_horse_power\", \"rejets_co2\", \"nbplaces\", \"nbportes\", \"categorie\"\n",
    "    ).show(10, truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3f9c8054e2877c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_path = \"hdfs://namenode:9000/models/kmeans_model\"\n",
    "# model.save(model_path)\n",
    "# print(f\"Modèle K-Means sauvegardé dans HDFS : {model_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "160a524799d5443f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Charger le modèle K-Means\n",
    "loaded_model = KMeansModel.load(model_path)\n",
    "print(\"Modèle K-Means chargé avec succès.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cdbfa4d72460885"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout de la colonne manquante : rejets_co2\n",
      "Ajout de la colonne manquante : bonus_malus\n",
      "\n",
      "--- Données imputées hiérarchiquement ---\n",
      "+----------+--------------+---------------+---------+-----------+--------+--------+-------+--------+-------+------------------+-----------------+\n",
      "|marque    |modele        |immatriculation|puissance|longueur   |nbplaces|nbportes|couleur|occasion|prix   |rejets_co2        |bonus_malus      |\n",
      "+----------+--------------+---------------+---------+-----------+--------+--------+-------+--------+-------+------------------+-----------------+\n",
      "|marque    |nom           |immatriculation|NULL     |longueur   |NULL    |NULL    |couleur|NULL    |NULL   |62.1527272727273  |-2194.21320754717|\n",
      "|Renault   |Laguna 2.0T   |3176 TS 67     |170      |longue     |5       |5       |blanc  |false   |27300.0|62.1527272727273  |-2194.21320754717|\n",
      "|Volvo     |S80 T6        |3721 QS 49     |272      |tr�s longue|5       |5       |noir   |false   |50500.0|62.1527272727273  |-2194.21320754717|\n",
      "|Volkswagen|Golf 2.0 FSI  |9099 UV 26     |150      |moyenne    |5       |5       |gris   |true    |16029.0|62.1527272727273  |-2194.21320754717|\n",
      "|Peugeot   |1007 1.4      |3563 LA 55     |75       |courte     |5       |5       |blanc  |true    |9625.0 |62.1527272727273  |-2194.21320754717|\n",
      "|Audi      |A2 1.4        |6963 AX 34     |75       |courte     |5       |5       |gris   |false   |18310.0|62.1527272727273  |-2194.21320754717|\n",
      "|Skoda     |Superb 2.8 V6 |5592 HQ 89     |193      |tr�s longue|5       |5       |bleu   |false   |31790.0|62.1527272727273  |-2194.21320754717|\n",
      "|Renault   |Megane 2.0 16V|674 CE 26      |135      |moyenne    |5       |5       |gris   |false   |22350.0|62.1527272727273  |-2194.21320754717|\n",
      "|Mercedes  |A200          |1756 PR 31     |136      |moyenne    |5       |5       |noir   |true    |18130.0|62.1527272727273  |-2194.21320754717|\n",
      "|BMW       |120i          |6705 GX 50     |150      |moyenne    |5       |5       |noir   |true    |25060.0|38.699999999999996|-6000.0          |\n",
      "+----------+--------------+---------------+---------+-----------+--------+--------+-------+--------+-------+------------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4752.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 526.0 failed 1 times, most recent failure: Lost task 0.0 in stage 526.0 (TID 552) (74c859225edd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3681/0x00007f19dd1ec638`: (struct<prix:double,rejets_co2:double,bonus_malus:double,nbplaces:double,nbportes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3681/0x00007f19dd1ec638`: (struct<prix:double,rejets_co2:double,bonus_malus:double,nbplaces:double,nbportes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 98\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Normalisation des données\u001B[39;00m\n\u001B[1;32m     97\u001B[0m scaler \u001B[38;5;241m=\u001B[39m MinMaxScaler(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures_raw\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 98\u001B[0m scaler_model \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43massembled_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m normalized_df \u001B[38;5;241m=\u001B[39m scaler_model\u001B[38;5;241m.\u001B[39mtransform(assembled_df)\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# Étape 5 : Charger et appliquer le modèle K-Means\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o4752.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 526.0 failed 1 times, most recent failure: Lost task 0.0 in stage 526.0 (TID 552) (74c859225edd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3681/0x00007f19dd1ec638`: (struct<prix:double,rejets_co2:double,bonus_malus:double,nbplaces:double,nbportes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3681/0x00007f19dd1ec638`: (struct<prix:double,rejets_co2:double,bonus_malus:double,nbplaces:double,nbportes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Imputation Hiérarchique et Clustering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Charger les données d'entraînement\n",
    "training_data = spark.sql(\"SELECT * FROM catalogue_co2_merge_processed\")\n",
    "\n",
    "# Charger les données de prédiction\n",
    "new_data_df = spark.sql(\"SELECT * FROM immatriculations_data\")\n",
    "\n",
    "# Renommer `nom` en `modele` pour aligner les colonnes\n",
    "new_data_df = new_data_df.withColumnRenamed(\"nom\", \"modele\")\n",
    "\n",
    "# Vérifier et ajouter les colonnes manquantes\n",
    "columns_to_check = [\"rejets_co2\", \"bonus_malus\"]\n",
    "for col_name in columns_to_check:\n",
    "    if col_name not in new_data_df.columns:\n",
    "        print(f\"Ajout de la colonne manquante : {col_name}\")\n",
    "        new_data_df = new_data_df.withColumn(col_name, lit(None).cast(\"double\"))\n",
    "\n",
    "# Étape 1 : Calcul des moyennes\n",
    "# Moyennes par modèle\n",
    "avg_by_model = training_data.groupBy(\"modele\").agg(\n",
    "    F.mean(\"rejets_co2\").alias(\"avg_rejets_co2_model\"),\n",
    "    F.mean(\"bonus_malus\").alias(\"avg_bonus_malus_model\")\n",
    ")\n",
    "\n",
    "# Moyennes par marque\n",
    "avg_by_marque = training_data.groupBy(\"marque\").agg(\n",
    "    F.mean(\"rejets_co2\").alias(\"avg_rejets_co2_marque\"),\n",
    "    F.mean(\"bonus_malus\").alias(\"avg_bonus_malus_marque\")\n",
    ")\n",
    "\n",
    "# Moyennes globales\n",
    "avg_global = training_data.agg(\n",
    "    F.mean(\"rejets_co2\").alias(\"avg_rejets_co2_global\"),\n",
    "    F.mean(\"bonus_malus\").alias(\"avg_bonus_malus_global\")\n",
    ").collect()[0]\n",
    "\n",
    "# Ajouter les moyennes globales dans les données de prédiction\n",
    "new_data_df = new_data_df.withColumn(\"avg_rejets_co2_global\", lit(avg_global[\"avg_rejets_co2_global\"]))\n",
    "new_data_df = new_data_df.withColumn(\"avg_bonus_malus_global\", lit(avg_global[\"avg_bonus_malus_global\"]))\n",
    "\n",
    "# Étape 2 : Joindre les moyennes\n",
    "# Joindre les moyennes par modèle\n",
    "new_data_df = new_data_df.join(avg_by_model, on=\"modele\", how=\"left\")\n",
    "\n",
    "# Joindre les moyennes par marque\n",
    "new_data_df = new_data_df.join(avg_by_marque, on=\"marque\", how=\"left\")\n",
    "\n",
    "# Étape 3 : Remplir les colonnes manquantes hiérarchiquement\n",
    "new_data_df = new_data_df.withColumn(\n",
    "    \"rejets_co2\",\n",
    "    when(col(\"rejets_co2\").isNotNull(), col(\"rejets_co2\"))  # Valeur existante\n",
    "    .when(col(\"avg_rejets_co2_model\").isNotNull(), col(\"avg_rejets_co2_model\"))  # Par modèle\n",
    "    .when(col(\"avg_rejets_co2_marque\").isNotNull(), col(\"avg_rejets_co2_marque\"))  # Par marque\n",
    "    .otherwise(col(\"avg_rejets_co2_global\"))  # Globale\n",
    ")\n",
    "\n",
    "new_data_df = new_data_df.withColumn(\n",
    "    \"bonus_malus\",\n",
    "    when(col(\"bonus_malus\").isNotNull(), col(\"bonus_malus\"))  # Valeur existante\n",
    "    .when(col(\"avg_bonus_malus_model\").isNotNull(), col(\"avg_bonus_malus_model\"))  # Par modèle\n",
    "    .when(col(\"avg_bonus_malus_marque\").isNotNull(), col(\"avg_bonus_malus_marque\"))  # Par marque\n",
    "    .otherwise(col(\"avg_bonus_malus_global\"))  # Globale\n",
    ")\n",
    "\n",
    "# Supprimer les colonnes intermédiaires de moyennes\n",
    "new_data_df = new_data_df.drop(\n",
    "    \"avg_rejets_co2_model\", \"avg_bonus_malus_model\",\n",
    "    \"avg_rejets_co2_marque\", \"avg_bonus_malus_marque\",\n",
    "    \"avg_rejets_co2_global\", \"avg_bonus_malus_global\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Données imputées hiérarchiquement ---\")\n",
    "new_data_df.show(10, truncate=False)\n",
    "\n",
    "# Étape 4 : Préparer les données pour le modèle\n",
    "numerical_cols = ['prix', 'rejets_co2', 'bonus_malus', 'nbplaces', 'nbportes']\n",
    "\n",
    "# Conversion explicite en type double\n",
    "for col_name in numerical_cols:\n",
    "    new_data_df = new_data_df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "# Assemblage des colonnes en vecteur de caractéristiques\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features_raw\")\n",
    "assembled_df = assembler.transform(new_data_df)\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "normalized_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "# Étape 5 : Charger et appliquer le modèle K-Means\n",
    "kmeans_model = KMeansModel.load(\"hdfs://namenode:9000/models/kmeans_model\")\n",
    "predicted_df = kmeans_model.transform(normalized_df)\n",
    "\n",
    "# Ajouter des catégories basées sur les prédictions\n",
    "predicted_df = predicted_df.withColumn(\n",
    "    \"categorie\",\n",
    "    when(col(\"prediction\") == 1, \"Berlines compactes économiques\")\n",
    "    .when(col(\"prediction\") == 6, \"Compactes premium\")\n",
    "    .when(col(\"prediction\") == 5, \"Monospaces familiaux 7 places\")\n",
    "    .when(col(\"prediction\") == 4, \"Berlines de luxe haut de gamme\")\n",
    "    .when(col(\"prediction\") == 8, \"Voitures sportives haut de gamme\")\n",
    "    .when(col(\"prediction\") == 2, \"Citadines économiques compactes\")\n",
    "    .when(col(\"prediction\") == 0, \"Berlines routières intermédiaires\")\n",
    "    .when(col(\"prediction\") == 3, \"Micro-citadines économiques\")\n",
    "    .when(col(\"prediction\") == 7, \"Monospaces compacts économiques\")\n",
    "    .otherwise(\"Non défini\")\n",
    ")\n",
    "\n",
    "# Étape 6 : Résumé des catégories\n",
    "print(\"\\n--- Résumé des catégories par prédiction ---\")\n",
    "predicted_df.groupBy(\"prediction\", \"categorie\").count().show()\n",
    "\n",
    "# Étape 7 : Sauvegarder les résultats\n",
    "output_path = \"hdfs://namenode:9000/output/clustered_results_hierarchical.csv\"\n",
    "predicted_df.select(\n",
    "    \"marque\", \"modele\", *numerical_cols, \"prediction\", \"categorie\"\n",
    ").write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "\n",
    "print(f\"Résultats sauvegardés dans : {output_path}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T11:26:25.014878900Z",
     "start_time": "2024-11-21T11:26:24.156333300Z"
    }
   },
   "id": "cf90705a44a9c696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9267153c71b387f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
